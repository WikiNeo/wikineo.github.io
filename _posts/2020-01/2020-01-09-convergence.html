---
title:  "Convergence"
published: true
tags: Math
---

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<p>
    <b>Definition 2.4.1</b>
    A sequence \((a_n)\) is increasing if \(a_n \leqslant a_{n+1}\) for all \(n
    \in N\) and decreasing if \(a_n \geqslant a_{n+1}\) for all \(n \in N\). A
    sequence is monotone if it is either increasing of decreasing.
</p>

<p>
    <b>Theorem 2.4.2</b>
    <b>Monotone Convergence Theorem</b>. If a sequence is monotone and bounded,
    then it converges.
</p>

<p>

    <i>Proof.</i> Let \((a_n)\) be monotone and bounded. To prove \((a_n)\)
    converges using the definition of convergence, we are going to need a
    candidate for the limit. Let's assume the sequence is increasing (the
    decreasing case is handled similarly), and consider the set of points
    \(\{a_n : n \in N\}\). By assumption, this set is bounded, so we can let
    $$
    s = sup\{a_n : n \in N\}
    $$
    It seems reasonable to claim that \(\lim a_n = s\)
</p>

<p>
    To prove this, let \(\epsilon > 0\). Because \(s\) is the least upper bound
    for \(\{a_n : n \in N\}\), \(s - \epsilon\) is not an upper bound, so there
    exists a point in the sequence \(a_N\) such that \(s - \epsilon < a_N\).
    Now, the fact that \(a_n\) is increasing implies that if \(n \geqslant N\),
    then \(a_N \leqslant a_n\). Hence,
    $$
    s - \epsilon < a_N \leqslant a_n \leqslant s < s + \epsilon
    $$
    which implies \(|a_n - s| < \epsilon\), as desired.
</p>

<p>

    The Monotone Convergence Theorem is extremely useful for the study of
    infinite series, largely because it asserts the convergence of a sequence
    without explicit mention of the actual limit. 
</p>

<p>
    <b>Definition 2.4.3</b>
    <b>Convergence of a Series</b>. Let \((b_n)\) be a sequence. An infinite
    series is a formal expression of the form
    $$
    \sum_{i=1}^\infty b_n = b_1 + b_2 + b_3 + b_4 + b_5 + \cdot \cdot \cdot
    $$
    We define the corresponding sequence of partial sums \((s_m)\) by
    $$
    s_m = b_1 + b_2 + b_3 + \cdot \cdot \cdot + b_m
    $$
    and say that the series \(\sum_{n=1}^\infty b_n \) converges to \(B\) if the
    sequence \((s_m)\) converges to \(B\). In this case, we write
    \(\sum_{n=1}^\infty b_n = B\)
</p>


<p>
    <b>Theorem 2.4.6</b>
    <b>Cauchy Condensation Test</b>. Suppose \((b_n)\) is decreasing and
    satisfying \(b_n \geqslant 0\) for all \(n \in N\). Then, the series
    \(\sum_{n=1}^\infty b_n\) converges if and only if the series
    $$
    \sum_{n=0}^\infty 2^n b_{2^n} = b_1 + 2b_2 + 4b_4 + 8b_8 + 16b_{16} + \cdot
    \cdot \cdot
    $$
    converges.
</p>

<p>

    <i>Proof.</i> First, assume that \(\sum_{n=0}^\infty 2^n b_{2^n}\)
    converges. Then the partial sums
    $$
    t_k = b_1 + 2b_2 + 4b4 + \cdot \cdot \cdot + 2^k b_{2^k}
    $$
    are bounded; that is, there exists an \(M > 0\) such that \(t_k \leqslant
    M\) for all \(k \in N\). We want to prove that \(\sum_{n=1}^\infty b_n\)
    converges. Because \(b_n \geqslant 0\), we know that the partial sums are
    increasing, so we only need to show that
    $$
    s_m = b_1 + b_2 + b_3  \cdot \cdot \cdot + b_m
    $$
    is bounded.
</p>

<p>

    Fix \(m\) and let \(k\) be large enough to ensure \(m \leqslant 2^{k+1} -
    1\). Then, \(s_m \leqslant s_{2^{k+1} - 1}\) and
    $$
    \begin{align}
    s_{2^{k+1} - 1} &= b_1 + (b_2 + b_3) + (b_4 + b_5 + b_6 + b_7) + \cdot \cdot
    \cdot + (b_{2^k} + \cdot \cdot \cdot + b_{2^{k+1} - 1})
    &\leqslant b_1 + (b_2 + b_2) + (b_4 + b_4 + b_4 + b_4) + \cdot \cdot \cdot +
    (b_{2^k} + \cdot \cdot \cdot + b_{2^k} )
    &= b_1 + 2b_2 + 4b_4 + \cdot \cdot \cdot + 2^k b_{2^k} = t_k
    \end{align}
    $$
    Thus, \(s_m \leqslant t_k \leqslant M\). By the Monotone Convergence
    Theorem, we can conclude that \(\sum_{n=1}^\infty b_n\) converges.
</p>

<p>
    <b>Corollary 2.4.7</b> 
    The series \(\sum_{n=1}^\infty 1/n^p\) converges if and only if \(p > 1\)
</p>

<p>
    <b>References</b>: Understanding Analysis By Stephen Abbott
</p>
